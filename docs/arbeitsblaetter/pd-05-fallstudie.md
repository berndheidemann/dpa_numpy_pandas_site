# Pandas â€“ Fallstudie Shark Attacks

## Lernziele

Nach Bearbeitung dieses Arbeitsblatts kannst du:

- einen komplexen, realen Datensatz selbststÃ¤ndig analysieren
- Datenbereinigung bei unstrukturierten Daten durchfÃ¼hren
- fortgeschrittene Pandas-Techniken anwenden
- aussagekrÃ¤ftige Erkenntnisse aus Daten ableiten

!!! note "Begleitende InfoblÃ¤tter"
    - [:material-book-open-variant: Pandas Grundlagen](../infoblaetter/pandas-grundlagen.md)
    - [:material-book-open-variant: Pandas Aggregation](../infoblaetter/pandas-aggregation.md)
    - [:material-book-open-variant: Datenbereinigung](../infoblaetter/datenbereinigung.md)

---

## EinfÃ¼hrung

Der Global Shark Attack File (GSAF) ist eine Datenbank aller dokumentierten Haiangriffe weltweit. Der Datensatz enthÃ¤lt viele fehlende Werte und unstrukturierte Textdaten â€“ eine realistische Herausforderung!

```kroki-plantuml
@startuml
!theme plain
skinparam backgroundColor transparent

rectangle "Analyse-Workflow" {
    rectangle "1. Laden & Verstehen" as step1 #lightcoral {
        rectangle "â€¢ Shape, Spalten\nâ€¢ Datentypen\nâ€¢ Fehlende Werte" as s1
    }
    
    rectangle "2. Bereinigen" as step2 #lightyellow {
        rectangle "â€¢ Spalten auswÃ¤hlen\nâ€¢ NaN behandeln\nâ€¢ Typen korrigieren" as s2
    }
    
    rectangle "3. Analysieren" as step3 #lightblue {
        rectangle "â€¢ Verteilungen\nâ€¢ Trends\nâ€¢ Muster" as s3
    }
    
    rectangle "4. Erkenntnisse" as step4 #lightgreen {
        rectangle "â€¢ Zusammenfassen\nâ€¢ Visualisieren\nâ€¢ Interpretieren" as s4
    }
}

step1 --> step2
step2 --> step3
step3 --> step4
@enduml
```

---

## Aufgaben

### Aufgabe 1 â€“ Daten laden und ersten Ãœberblick gewinnen

- [ ] **Lade den Datensatz:**
    ```python
    import pandas as pd
    import numpy as np
    
    # Datensatz laden
    sharks = pd.read_csv('../assets/files/global_shark_attacks.csv',
                         encoding='latin-1')  # Encoding fÃ¼r Sonderzeichen
    
    print(f"Shape: {sharks.shape}")
    print(f"Spalten: {sharks.columns.tolist()}")
    ```

- [ ] **Erste Zeilen anschauen:**
    ```python
    print("\nErste 3 Zeilen:")
    print(sharks.head(3).T)  # Transponiert fÃ¼r bessere Lesbarkeit
    ```

- [ ] **Datentypen und fehlende Werte:**
    ```python
    print("\n=== Datensatz-Info ===")
    sharks.info()
    
    print("\n=== Fehlende Werte (Top 10) ===")
    missing = sharks.isnull().sum().sort_values(ascending=False)
    print(missing.head(10))
    print(f"\nGesamt fehlende Werte: {missing.sum()}")
    ```

---

### Aufgabe 2 â€“ Relevante Spalten auswÃ¤hlen

Der Datensatz hat viele Spalten. WÃ¤hle die wichtigsten aus.

- [ ] **Spalten identifizieren:**
    ```python
    # Typische wichtige Spalten
    # (Namen kÃ¶nnen je nach Datensatz-Version variieren!)
    print("Alle Spaltennamen:")
    for i, col in enumerate(sharks.columns):
        print(f"  {i}: {col}")
    ```

- [ ] **Arbeits-DataFrame erstellen:**
    ```python
    # Relevante Spalten auswÃ¤hlen (Namen anpassen falls nÃ¶tig!)
    # Typische Spalten: Year, Country, Area, Activity, Name, Sex, Age, Injury, Fatal
    
    # Versuche gÃ¤ngige Spaltennamen
    possible_cols = ['Year', 'Country', 'Area', 'Location', 'Activity', 
                     'Name', 'Sex', 'Age', 'Injury', 'Fatal (Y/N)', 
                     'Time', 'Species']
    
    # Nur vorhandene Spalten auswÃ¤hlen
    use_cols = [c for c in possible_cols if c in sharks.columns]
    print(f"Gefundene Spalten: {use_cols}")
    
    df = sharks[use_cols].copy()
    print(f"\nArbeits-DataFrame Shape: {df.shape}")
    ```

---

### Aufgabe 3 â€“ Daten bereinigen

- [ ] **Jahr bereinigen:**
    ```python
    # Jahr sollte numerisch sein
    print("=== Jahr bereinigen ===")
    print(f"Datentyp vorher: {df['Year'].dtype}")
    print(f"Beispielwerte: {df['Year'].head(10).tolist()}")
    
    # Zu numerisch konvertieren (Fehler werden NaN)
    df['Year'] = pd.to_numeric(df['Year'], errors='coerce')
    
    # Unrealistische Jahre entfernen (vor 1800, nach aktuellem Jahr)
    import datetime
    current_year = datetime.datetime.now().year
    df = df[(df['Year'] >= 1800) & (df['Year'] <= current_year)]
    
    print(f"Nach Bereinigung: {df['Year'].min():.0f} - {df['Year'].max():.0f}")
    print(f"Anzahl nach Filter: {len(df)}")
    ```

- [ ] **Fatal (tÃ¶dlich) bereinigen:**
    ```python
    print("\n=== Fatal bereinigen ===")
    if 'Fatal (Y/N)' in df.columns:
        print("Werte vorher:")
        print(df['Fatal (Y/N)'].value_counts(dropna=False))
        
        # Standardisieren
        df['Fatal'] = df['Fatal (Y/N)'].str.upper().str.strip()
        df['Fatal'] = df['Fatal'].map({'Y': True, 'N': False})
        
        print("\nWerte nachher:")
        print(df['Fatal'].value_counts(dropna=False))
    ```

- [ ] **Alter bereinigen:**
    ```python
    print("\n=== Alter bereinigen ===")
    print(f"Datentyp: {df['Age'].dtype}")
    print(f"Beispiele: {df['Age'].dropna().head(10).tolist()}")
    
    # Zu numerisch konvertieren
    df['Age'] = pd.to_numeric(df['Age'], errors='coerce')
    
    # Unrealistische Alter entfernen
    df.loc[(df['Age'] < 0) | (df['Age'] > 100), 'Age'] = np.nan
    
    print(f"\nAlter-Statistik:")
    print(df['Age'].describe())
    ```

- [ ] **Geschlecht standardisieren:**
    ```python
    print("\n=== Geschlecht bereinigen ===")
    print("Werte vorher:")
    print(df['Sex'].value_counts(dropna=False))
    
    df['Sex'] = df['Sex'].str.upper().str.strip()
    df['Sex'] = df['Sex'].map({'M': 'Male', 'F': 'Female'})
    
    print("\nWerte nachher:")
    print(df['Sex'].value_counts(dropna=False))
    ```

---

### Aufgabe 4 â€“ Deskriptive Statistik

- [ ] **Grundstatistiken:**
    ```python
    print("=== Grundstatistiken ===")
    print(f"Anzahl Angriffe: {len(df)}")
    print(f"Zeitraum: {df['Year'].min():.0f} - {df['Year'].max():.0f}")
    print(f"Anzahl LÃ¤nder: {df['Country'].nunique()}")
    ```

- [ ] **Top LÃ¤nder:**
    ```python
    print("\n=== Top 10 LÃ¤nder ===")
    top_countries = df['Country'].value_counts().head(10)
    print(top_countries)
    
    # Prozentsatz
    print(f"\nAnteil Top 10: {top_countries.sum() / len(df) * 100:.1f}%")
    ```

- [ ] **HÃ¤ufigste AktivitÃ¤ten:**
    ```python
    print("\n=== Top 10 AktivitÃ¤ten ===")
    activities = df['Activity'].value_counts().head(10)
    print(activities)
    ```

- [ ] **Altersverteilung:**
    ```python
    print("\n=== Altersverteilung ===")
    print(df['Age'].describe())
    
    # Altersgruppen
    df['Age_Group'] = pd.cut(
        df['Age'],
        bins=[0, 10, 20, 30, 40, 50, 60, 100],
        labels=['0-10', '11-20', '21-30', '31-40', '41-50', '51-60', '60+']
    )
    
    print("\nNach Altersgruppe:")
    print(df['Age_Group'].value_counts().sort_index())
    ```

---

### Aufgabe 5 â€“ Zeitliche Trends

- [ ] **Angriffe pro Jahr:**
    ```python
    print("=== Zeitlicher Trend ===")
    
    # Angriffe pro Jahr
    yearly = df.groupby('Year').size()
    
    print("Letzte 10 Jahre:")
    print(yearly.tail(10))
    
    # Trend berechnen
    recent = yearly[yearly.index >= 2000]
    print(f"\nDurchschnitt 2000-heute: {recent.mean():.1f} Angriffe/Jahr")
    print(f"Max: {recent.max()} ({recent.idxmax():.0f})")
    print(f"Min: {recent.min()} ({recent.idxmin():.0f})")
    ```

- [ ] **Dekaden-Analyse:**
    ```python
    # Dekade erstellen
    df['Decade'] = (df['Year'] // 10) * 10
    
    decade_stats = df.groupby('Decade').agg(
        Angriffe=('Year', 'count'),
        Fatal_Prozent=('Fatal', lambda x: x.mean() * 100 if x.notna().any() else np.nan)
    ).round(1)
    
    print("\nAngriffe pro Dekade:")
    print(decade_stats[decade_stats.index >= 1900])
    ```

- [ ] **Saisonale Muster (falls Time vorhanden):**
    ```python
    # Monat extrahieren (falls Date-Spalte vorhanden)
    # Oder aus 'Date' String parsen
    
    # Beispiel-Ansatz:
    # df['Month'] = pd.to_datetime(df['Date'], errors='coerce').dt.month
    
    print("\nSaisonale Analyse erfordert Datums-Parsing")
    print("(Datensatz-spezifisch)")
    ```

---

### Aufgabe 6 â€“ TÃ¶dliche Angriffe analysieren

- [ ] **TÃ¶dlichkeitsrate:**
    ```python
    print("=== TÃ¶dlichkeit ===")
    
    # Gesamtrate
    fatal_rate = df['Fatal'].mean() * 100
    print(f"Gesamt-TÃ¶dlichkeitsrate: {fatal_rate:.1f}%")
    
    # Nach Dekade
    print("\nTÃ¶dlichkeit nach Dekade:")
    fatal_by_decade = df.groupby('Decade')['Fatal'].mean() * 100
    print(fatal_by_decade[fatal_by_decade.index >= 1950].round(1))
    ```

- [ ] **TÃ¶dlichkeit nach Land:**
    ```python
    print("\n=== TÃ¶dlichkeit nach Land (Top 10 nach Anzahl) ===")
    
    # Nur LÃ¤nder mit mindestens 50 Angriffen
    country_stats = df.groupby('Country').agg(
        Angriffe=('Year', 'count'),
        TÃ¶dlich=('Fatal', 'sum'),
        Rate=('Fatal', lambda x: x.mean() * 100)
    ).round(1)
    
    country_stats = country_stats[country_stats['Angriffe'] >= 50]
    country_stats = country_stats.sort_values('Angriffe', ascending=False)
    
    print(country_stats.head(10))
    ```

- [ ] **TÃ¶dlichkeit nach AktivitÃ¤t:**
    ```python
    print("\n=== TÃ¶dlichkeit nach AktivitÃ¤t (mind. 20 FÃ¤lle) ===")
    
    activity_stats = df.groupby('Activity').agg(
        Angriffe=('Year', 'count'),
        Rate=('Fatal', lambda x: x.mean() * 100)
    ).round(1)
    
    activity_stats = activity_stats[activity_stats['Angriffe'] >= 20]
    activity_stats = activity_stats.sort_values('Rate', ascending=False)
    
    print(activity_stats.head(10))
    ```

---

### Aufgabe 7 â€“ Tiefere Analysen

- [ ] **Geschlechtervergleich:**
    ```python
    print("=== Geschlechtervergleich ===")
    
    gender_stats = df.groupby('Sex').agg(
        Anzahl=('Year', 'count'),
        Durchschnittsalter=('Age', 'mean'),
        TÃ¶dlichkeit=('Fatal', lambda x: x.mean() * 100)
    ).round(1)
    
    # Anteil berechnen
    gender_stats['Anteil_%'] = (gender_stats['Anzahl'] / gender_stats['Anzahl'].sum() * 100).round(1)
    
    print(gender_stats)
    ```

- [ ] **Altersanalyse:**
    ```python
    print("\n=== Alter und TÃ¶dlichkeit ===")
    
    # TÃ¶dlichkeit nach Altersgruppe
    age_fatal = df.groupby('Age_Group').agg(
        Anzahl=('Year', 'count'),
        TÃ¶dlichkeit=('Fatal', lambda x: x.mean() * 100)
    ).round(1)
    
    print(age_fatal)
    
    # Durchschnittsalter bei tÃ¶dlichen vs. nicht-tÃ¶dlichen
    print(f"\nDurchschnittsalter:")
    print(f"  TÃ¶dliche Angriffe: {df[df['Fatal'] == True]['Age'].mean():.1f}")
    print(f"  Nicht-tÃ¶dliche: {df[df['Fatal'] == False]['Age'].mean():.1f}")
    ```

- [ ] **LÃ¤nderprofile erstellen:**
    ```python
    print("\n=== LÃ¤nderprofile (Top 5) ===")
    
    top5_countries = df['Country'].value_counts().head(5).index.tolist()
    
    for country in top5_countries:
        subset = df[df['Country'] == country]
        print(f"\n{country}:")
        print(f"  Angriffe: {len(subset)}")
        print(f"  Zeitraum: {subset['Year'].min():.0f}-{subset['Year'].max():.0f}")
        print(f"  TÃ¶dlichkeit: {subset['Fatal'].mean() * 100:.1f}%")
        print(f"  Top-AktivitÃ¤t: {subset['Activity'].mode().iloc[0] if len(subset['Activity'].mode()) > 0 else 'N/A'}")
        print(f"  Durchschnittsalter: {subset['Age'].mean():.1f}")
    ```

---

### Aufgabe 8 â€“ Pivot-Tabellen erstellen

- [ ] **Kreuztabelle Land Ã— Dekade:**
    ```python
    print("=== Angriffe: Land Ã— Dekade ===")
    
    # Top 5 LÃ¤nder, ab 1950
    df_recent = df[(df['Decade'] >= 1950) & (df['Country'].isin(top5_countries))]
    
    pivot = pd.pivot_table(
        df_recent,
        values='Year',
        index='Country',
        columns='Decade',
        aggfunc='count',
        fill_value=0
    )
    
    print(pivot)
    ```

- [ ] **TÃ¶dlichkeit: Land Ã— AktivitÃ¤t:**
    ```python
    print("\n=== TÃ¶dlichkeit: Land Ã— AktivitÃ¤t (Top) ===")
    
    top_activities = df['Activity'].value_counts().head(5).index.tolist()
    df_filtered = df[df['Country'].isin(top5_countries) & df['Activity'].isin(top_activities)]
    
    pivot_fatal = pd.pivot_table(
        df_filtered,
        values='Fatal',
        index='Country',
        columns='Activity',
        aggfunc='mean'
    ) * 100
    
    print(pivot_fatal.round(1))
    ```

---

### Aufgabe 9 â€“ Erkenntnisse dokumentieren

- [ ] **Zusammenfassung erstellen:**
    
    Fasse deine wichtigsten Erkenntnisse zusammen:

    ```python
    print("=" * 50)
    print("ZUSAMMENFASSUNG: Global Shark Attacks")
    print("=" * 50)
    
    print(f"\nðŸ“Š DATENSATZ")
    print(f"   â€¢ {len(df):,} dokumentierte Angriffe")
    print(f"   â€¢ Zeitraum: {df['Year'].min():.0f} - {df['Year'].max():.0f}")
    print(f"   â€¢ {df['Country'].nunique()} LÃ¤nder")
    
    print(f"\nðŸ¦ˆ RISIKO")
    print(f"   â€¢ Gesamt-TÃ¶dlichkeitsrate: {df['Fatal'].mean() * 100:.1f}%")
    print(f"   â€¢ GefÃ¤hrlichstes Land: {country_stats['Rate'].idxmax()}")
    print(f"   â€¢ Sicherste AktivitÃ¤t: {activity_stats['Rate'].idxmin()}")
    
    print(f"\nðŸ‘¤ DEMOGRAFIE")
    print(f"   â€¢ Durchschnittsalter: {df['Age'].mean():.1f} Jahre")
    print(f"   â€¢ MÃ¤nneranteil: {(df['Sex'] == 'Male').sum() / df['Sex'].notna().sum() * 100:.1f}%")
    
    print(f"\nðŸ“ˆ TRENDS")
    recent_decade = df[df['Year'] >= 2010]
    older_decade = df[(df['Year'] >= 1990) & (df['Year'] < 2000)]
    print(f"   â€¢ Angriffe 1990er: {len(older_decade)} / Dekade")
    print(f"   â€¢ Angriffe 2010er+: {len(recent_decade)} / Dekade")
    ```

---

## Bonus-Aufgaben

??? tip "FÃ¼r Fortgeschrittene"
    **A) Hai-Arten analysieren:**
    - Extrahiere Hai-Arten aus der Species-Spalte
    - Welche Art ist am gefÃ¤hrlichsten?
    
    **B) Text Mining:**
    - Analysiere die Injury-Beschreibungen
    - Welche KÃ¶rperteile werden am hÃ¤ufigsten verletzt?
    
    **C) Geographische Analyse:**
    - Gruppiere nach Regionen (Area/Location)
    - Gibt es Hotspots innerhalb der Top-LÃ¤nder?
    
    **D) Vorhersage-Modell:**
    - KÃ¶nnen wir basierend auf AktivitÃ¤t, Ort, Alter vorhersagen ob ein Angriff tÃ¶dlich ist?

---

## Zusammenfassung

!!! success "Das hast du gelernt"
    - **Reale Daten** sind messy â€“ Bereinigung ist essentiell
    - **Encoding-Probleme** mit `encoding='latin-1'` lÃ¶sen
    - **Typenkonvertierung** mit `pd.to_numeric(errors='coerce')`
    - **Aggregation** mit `groupby` und `pivot_table`
    - **Explorative Analyse** systematisch durchfÃ¼hren
    - **Erkenntnisse** zusammenfassen und interpretieren

---

??? question "Selbstkontrolle"
    1. Warum verwendet man `errors='coerce'` bei der Typkonvertierung?
    2. Wie berechnet man die TÃ¶dlichkeitsrate pro Gruppe?
    3. Was bedeutet `dropna=False` bei `value_counts()`?
    4. Wann ist ein Datensatz "sauber genug" fÃ¼r die Analyse?
    
    ??? success "Antworten"
        1. UngÃ¼ltige Werte werden zu NaN statt einen Fehler zu werfen â€“ wichtig bei unstrukturierten Daten
        2. `.groupby('Gruppe')['Fatal'].mean() * 100` â€“ mean() auf True/False gibt den Anteil True
        3. NaN-Werte werden auch gezÃ¤hlt statt ignoriert â€“ wichtig um fehlende Werte zu sehen
        4. Wenn die verbleibenden Probleme die Analyse nicht verfÃ¤lschen und die Kernfragen beantwortet werden kÃ¶nnen â€“ Perfekte Daten gibt es selten!
